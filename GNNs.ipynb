{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc48223-a825-4a79-802e-cb9c866c3c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric\n",
    "from torchsummary import summary\n",
    "import torch_scatter\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdmolops\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff97141-1374-4b64-9241-8ccdbd5f876e",
   "metadata": {},
   "source": [
    "# Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6052d602-62e2-42f6-9ca7-16f8a1adc853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LigandDataset():\n",
    "    def __init__(self, atom_list, natom_list, edge_index_list, edge_feats_list, y_list, denticities_list):\n",
    "        self.atom_list = atom_list\n",
    "        self.natom_list = natom_list\n",
    "        self.edge_index_list = edge_index_list\n",
    "        self.edge_feats_list = edge_feats_list\n",
    "        self.y_list = y_list\n",
    "        self.denticities_list = denticities_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.atom_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return Data(x=torch.Tensor(self.atom_list[idx]),\n",
    "                    natoms=torch.Tensor([self.natom_list[idx]]),\n",
    "                    edge_index=torch.Tensor(np.array(self.edge_index_list[idx])),\n",
    "                    edge_attr=torch.Tensor(self.edge_feats_list[idx]),\n",
    "                    y=torch.Tensor(self.y_list[idx]).unsqueeze(1).to(torch.long),\n",
    "                    denticity=torch.Tensor([self.denticities_list[idx]]),\n",
    "                    # y=torch.nn.functional.one_hot(torch.Tensor(self.y_list[idx]).to(torch.long), num_classes=2) # one-hot\n",
    "                   )\n",
    "\n",
    "train_data = torch.load('data/train_dataset.pt')\n",
    "test_data = torch.load('data/test_dataset.pt')\n",
    "val_data = torch.load('data/val_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87417d0e-6859-479b-8e61-26b51629d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a302459c-ad1d-47ad-a6fb-c240ca09b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_loss(preds: torch.Tensor, labels: torch.Tensor, inds: torch.Tensor):\n",
    "    \"\"\" \n",
    "    Computes a cross-entropy loss for each atom.\n",
    "    Then, computes the mean of that loss for each ligand, and then across all ligands in the batch\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : torch.Tensor (N,1)\n",
    "        Atom-wise predicted logits for not-being or being a coordinating atom\n",
    "    labels : torch.Tensor (N,1)\n",
    "        Atom-wise labels for whether it isn't or is a coordinating atom\n",
    "    inds : torch.Tensor (batch_size+1)\n",
    "        The indices defining the ligands within each batch. Uses the batch.ptr generated by the torch_geometric dataloader.\n",
    "    Return\n",
    "    ------\n",
    "    torch.Tensor (1,)\n",
    "        Mean batch loss\n",
    "    \"\"\"\n",
    "    # this is (N,2) for some reason\n",
    "    loss_per_node = torch.nn.functional.binary_cross_entropy(preds, labels, reduction='none',\n",
    "                                                             weight=torch.Tensor([1]).to(0))\n",
    "    # Compute the mean cross-entropy across each individual graph, then the mean across the entire batch\n",
    "    # graph_sizes = torch.diff(inds)\n",
    "    # segment_ids = torch.repeat_interleave(torch.arange(len(graph_sizes), device=preds.device), graph_sizes)\n",
    "    # graph_losses = torch_scatter.scatter_mean(loss_per_node, segment_ids, dim=0)\n",
    "    # return graph_losses.mean()\n",
    "    \n",
    "    # Experimenting with averaging negative and positive losses\n",
    "    # Note: this does not average over graphs\n",
    "    # neg_loss = loss_per_node[labels[:,0].nonzero()]\n",
    "    # pos_loss = loss_per_node[labels[:,1].nonzero()]\n",
    "    # return neg_loss.mean() + pos_loss.mean()\n",
    "    \n",
    "    ## Average negative and positive losses separately per graph\n",
    "    graph_sizes = torch.diff(inds)\n",
    "    # Get how many ones/zeros are in each individual graph\n",
    "    num_ones_per_graph = torch.Tensor([len(labels[inds[i-1]:inds[i]].nonzero()) for i in range(1,len(inds))],\n",
    "                                     ).to(torch.long)\n",
    "    num_zeros_per_graph = torch.Tensor([len(torch.where(labels[inds[i-1]:inds[i]]==0)[0]) for i in range(1,len(inds))],\n",
    "                                     ).to(torch.long)\n",
    "    ones_seg_ids = torch.repeat_interleave(torch.arange(len(num_ones_per_graph)), num_ones_per_graph).to(preds.device)\n",
    "    zeros_seg_ids = torch.repeat_interleave(torch.arange(len(num_zeros_per_graph)), num_zeros_per_graph).to(preds.device)\n",
    "    # compute mean loss for each pos/neg for each graph\n",
    "    pos_loss = torch_scatter.scatter_mean(loss_per_node[labels.flatten().nonzero().flatten()], ones_seg_ids, dim=0)\n",
    "    neg_loss = torch_scatter.scatter_mean(loss_per_node[torch.where(labels==0)[0]], zeros_seg_ids, dim=0)\n",
    "    combined_loss_per_graph = pos_loss + neg_loss # element-wise for each graph\n",
    "\n",
    "    # pred_num_one = torch_scatter.scatter_add(loss_per_node[labels.flatten().nonzero().flatten()], ones_seg_ids, dim=0)\n",
    "    # pred_num_zero = torch_scatter.scatter_add(loss_per_node[torch.where(labels==0)[0]], zeros_seg_ids, dim=0)\n",
    "    \n",
    "    return (combined_loss_per_graph.mean())\n",
    "            # + 0.5*torch.mean(torch.square(pred_num_one/denticities - 1))\n",
    "            # + 0.01*torch.mean(torch.square(pred_num_zero/(natoms-denticities))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "101e6c30-1867-446b-8b3f-a70e1722e28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss for all 0 predictions: 1e+02\n",
      "Val Loss for opposite predictions: 2e+02\n",
      "Epoch: 1 | Avg Train Loss: 1.21 | Avg Val Loss: 0.933\n",
      "Epoch: 2 | Avg Train Loss: 1.05 | Avg Val Loss: 0.776\n",
      "Epoch: 3 | Avg Train Loss: 1.02 | Avg Val Loss: 0.723\n",
      "Epoch: 4 | Avg Train Loss: 0.992 | Avg Val Loss: 0.699\n",
      "Epoch: 5 | Avg Train Loss: 0.977 | Avg Val Loss: 0.681\n",
      "Epoch: 6 | Avg Train Loss: 0.971 | Avg Val Loss: 0.669\n",
      "Epoch: 7 | Avg Train Loss: 0.959 | Avg Val Loss: 0.648\n",
      "Epoch: 8 | Avg Train Loss: 0.953 | Avg Val Loss: 0.65\n",
      "Epoch: 9 | Avg Train Loss: 0.951 | Avg Val Loss: 0.652\n",
      "Epoch: 10 | Avg Train Loss: 0.953 | Avg Val Loss: 0.655\n",
      "Epoch: 11 | Avg Train Loss: 0.955 | Avg Val Loss: 0.654\n",
      "Epoch: 12 | Avg Train Loss: 0.955 | Avg Val Loss: 0.653\n"
     ]
    }
   ],
   "source": [
    "gat = torch_geometric.nn.GAT(-1, 20, num_layers=2, out_channels=1, dropout=0.5)\n",
    "optimizer = torch.optim.Adam(gat.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_epochs = 50\n",
    "gat.to(device)\n",
    "\n",
    "train_epoch_losses = []\n",
    "val_epoch_losses = []\n",
    "\n",
    "# what is the loss if we predict 0s for everything\n",
    "pred_0_loss_val = 0\n",
    "pred_opp_loss_val = 0\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_loader):\n",
    "        batch.to(device)\n",
    "        out_probs = torch.zeros(batch.y.shape, dtype=torch.float64).to(device)\n",
    "        loss = compute_batch_loss(out_probs, batch.y.to(torch.float64), batch.ptr)\n",
    "        pred_0_loss_val += loss.item()\n",
    "pred_0_loss_val = pred_0_loss_val / (i+1)\n",
    "\n",
    "# What is the loss if we predict the opposite for everything\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_loader):\n",
    "        batch.to(device)\n",
    "        out_probs = 1-batch.y.to(torch.float64).to(device)\n",
    "        loss = compute_batch_loss(out_probs, batch.y.to(torch.float64), batch.ptr)\n",
    "        pred_opp_loss_val += loss.item()\n",
    "pred_opp_loss_val = pred_opp_loss_val / (i+1)\n",
    "print(f'Val Loss for all 0 predictions: {pred_0_loss_val:.3}')\n",
    "print(f'Val Loss for opposite predictions: {pred_opp_loss_val:.3}')\n",
    "\n",
    "best_loss = 10000\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    gat.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch.to(device)\n",
    "\n",
    "        out_logits = gat(x=batch.x, edge_index=batch.edge_index.to(torch.int64), edge_attr=batch.edge_attr)\n",
    "        out_probs = torch.nn.functional.sigmoid(out_logits)\n",
    "        loss = compute_batch_loss(out_probs, batch.y.to(torch.float32), batch.ptr)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_train_loss = epoch_train_loss / (i+1)\n",
    "    train_epoch_losses.append(epoch_train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    epoch_val_loss = 0\n",
    "    gat.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            batch.to(0)\n",
    "            out_logits = gat(x=batch.x, edge_index=batch.edge_index.to(torch.int64), edge_attr=batch.edge_attr)\n",
    "            out_probs = torch.nn.functional.sigmoid(out_logits)\n",
    "            loss = compute_batch_loss(out_probs, batch.y.to(torch.float32), batch.ptr)\n",
    "            epoch_val_loss += loss.item()\n",
    "    epoch_val_loss = epoch_val_loss / (i+1)\n",
    "    val_epoch_losses.append(epoch_val_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} | Avg Train Loss: {epoch_train_loss:.3} | Avg Val Loss: {epoch_val_loss:.3}')\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch_val_loss < best_loss:\n",
    "        best_loss = epoch_val_loss\n",
    "        best_model_weights = copy.deepcopy(gat.state_dict())  # Deep copy here      \n",
    "        patience = 5  # Reset patience counter\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            break\n",
    "\n",
    "    # Load the best model weights\n",
    "    gat.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a83513-582f-459b-ade3-a8445bb14f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(out_probs.detach().cpu().numpy(), 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e4779-1318-4f3f-acc2-487a7ae6ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epoch+1), train_epoch_losses, label='Train Loss')\n",
    "plt.plot(range(epoch+1), val_epoch_losses, label='Val Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09ccb612-4e92-4af5-b2de-b5042ddc0553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8008538877710383\n"
     ]
    }
   ],
   "source": [
    "gat.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        batch.to(0)\n",
    "        out_logits = gat(x=batch.x, edge_index=batch.edge_index.to(torch.int64), edge_attr=batch.edge_attr)\n",
    "        out_probs = torch.nn.functional.sigmoid(out_logits)\n",
    "        loss = compute_batch_loss(out_probs, batch.y.to(torch.float32), batch.ptr)\n",
    "        test_loss += loss.item()\n",
    "    test_loss /= (i+1)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0debbc5d-9b26-4892-8fd8-6b2a8b69980b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─ReLU: 1-1                              --\n",
      "├─ModuleList: 1-2                        --\n",
      "|    └─GATConv: 2-1                      --\n",
      "|    |    └─SumAggregation: 3-1          --\n",
      "|    |    └─Linear: 3-2                  20\n",
      "|    └─GATConv: 2-2                      --\n",
      "|    |    └─SumAggregation: 3-3          --\n",
      "|    |    └─Linear: 3-4                  4\n",
      "=================================================================\n",
      "Total params: 24\n",
      "Trainable params: 24\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─ReLU: 1-1                              --\n",
       "├─ModuleList: 1-2                        --\n",
       "|    └─GATConv: 2-1                      --\n",
       "|    |    └─SumAggregation: 3-1          --\n",
       "|    |    └─Linear: 3-2                  20\n",
       "|    └─GATConv: 2-2                      --\n",
       "|    |    └─SumAggregation: 3-3          --\n",
       "|    |    └─Linear: 3-4                  4\n",
       "=================================================================\n",
       "Total params: 24\n",
       "Trainable params: 24\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(gat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b2045-a9d0-47e3-8164-2473fbad6af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
